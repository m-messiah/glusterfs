Проблема GlusterFS в его полной распределённости.

В основном, GlusterFS поддерживает две схемы хранения файлов:

+ afr - когда файлы полностью дублируются по всем томам
+ dht - когда кусочки файлов распрделяются по нодам, в зависимости от некоторой хэширующей функции.

На кластере же, то и дело возникают задачи, когда скорость обращения к некоторым файлам гораздо важнее, нежели к другимю
Так как вычислительные узлы и ноды Glusterfs это одни и те де компьютеры - возникает естественное желание переместить "нужные файлы" ближе к исполнителю, в идеале - читать и записывать их в его локальной системе и только в случае надобности среплицировать на другие ноды.

В нынешней разработке DHT GlusterFS есть возможность при распределении файлов использовать преимущественно локальное хранилище (nufa scheduler), либо заранее указать маппинг шаблона имени файла к нужной ноде(switch scheduler).

Первый случай плох тем, что при определённых ситуациях, когда запись идет постоянно с одного вычислителя - файлы будут сохраняться локально, а остальные ноды - пустовать.

Второй случай плох, что перед сохранением файла будет необходимо знать номер своей ноды -  и его дописывать к имени файла (имя файла портится опять же).

Планируется разобраться с Nufa - как расставлять приоритет локальности.
При этом разобраться с switch scheduler - для начала, насколько вариабельны pattern'ы, потом - попытаться вынести эту же логику в расширенные аттрибуты, чтобы не менять имя.

Помимо этого, планируется внедрить агрессивное кэширование файлов "только для чтения" (есть подозрение, что это вытекает из предыдущей задачи).


